# **Udacity Self-Driving Car Nanodegree | Project 3** 

---

**Behavioral Cloning**
Self-driving cars faces numerous situations when drove on the road. Its essential that the car is trained on a real scenario with good driving behaviour. This will prepare the self-driving car to act correctly, independent of any instructions from a driver.

List of files in Repository:
1. ***model.py*** is the python script to create and train the model
2. ***drive.py*** is the python script for driving the car in autonomous mode on udacity simulator
4. ***video.py*** is the python script for creating the video out of images from the simulator.
5. ***model.h5*** is the trained Keras model
6. **video.mp4** is the video file output generated by running the car in simulator in autonomous mode.
7. ***writeup_report.md*** is the markdown for summarizing the methodology and results
8. Other Folders and files contains resources for the writeup.


### Dependencies
This lab requires:

* Udacity Simulator for creating the training data and simulating the trained model
* [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit)


Employed Neural Network Architecture, Training Data Set, and Methodology involved in training is thoroughly discussed in the writeup_report.md file.


### Details About Important Files In This Directory

### `model.py`

Usage of `model.py` requires you have python with the pre-requisites from [CarND Term1 Starter Kit](https://github.com/udacity/CarND-Term1-Starter-Kit) and data extracted through run in udacity simulator.

To train the model with your data, make sure to place the images in IMG folder and the driving_log.csv file in the root directory of this project. Use the below command to train the model with your data.

```sh
python model.py
```
This will save the model as model.h5

### `drive.py`

Usage of `drive.py` requires you have saved the trained model as an h5 file, i.e. `model.h5`. 
Once the model has been saved, it can be used with drive.py using this command:

```sh
python drive.py model.h5
```

The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection.

Note: There is known local system's setting issue with replacing "," with "." when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add "export LANG=en_US.utf8" to the bashrc file.

#### Saving a video of the autonomous agent

```sh
python drive.py model.h5 run1
```

The fourth argument, `run1`, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten.

```sh
ls run1

[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg
[2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg
...
```

The image file name is a timestamp of when the image was seen. This information is used by `video.py` to create a chronological video of the agent driving.

### `video.py`

```sh
python video.py run1
```

Creates a video based on images found in the `run1` directory. The name of the video will be the name of the directory followed by `'.mp4'`, so, in this case the video will be `run1.mp4`.

Optionally, one can specify the FPS (frames per second) of the video:

```sh
python video.py run1 --fps 48
```

Will run the video at 48 FPS. The default FPS is 60.
